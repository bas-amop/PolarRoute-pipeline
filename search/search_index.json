{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PolarRoute-pipeline PolarRoute-pipeline is a data pipeline used to automate the generation of ocean/sea-ice meshes (and optionally optimised routes) for ocean vessel route-planning. This data pipeline is one component of the British Antarctic Survey's Operational PolarRoute (OPR) project. Using this documentation Most of these docs are primarily aimed at developers of PolarRoute-pipeline or administrators who want to install and run a copy of the pipeline.","title":"Home"},{"location":"#polarroute-pipeline","text":"PolarRoute-pipeline is a data pipeline used to automate the generation of ocean/sea-ice meshes (and optionally optimised routes) for ocean vessel route-planning. This data pipeline is one component of the British Antarctic Survey's Operational PolarRoute (OPR) project.","title":"PolarRoute-pipeline"},{"location":"#using-this-documentation","text":"Most of these docs are primarily aimed at developers of PolarRoute-pipeline or administrators who want to install and run a copy of the pipeline.","title":"Using this documentation"},{"location":"about/","text":"PolarRoute-pipeline is a data pipeline used to automate the generation of ocean/sea-ice meshes (and optionally optimised routes) for ocean vessel route-planning. This data pipeline is one component of the British Antarctic Survey's Operational PolarRoute (OPR) project.","title":"About"},{"location":"development/","text":"Development Depends on: Python >=3.9 Clone the repository and create and activate a python virtual environment of your choice. Inside a virtual environment or machine: python -m pip install -r requirements.txt Release/Versioning Version numbers should be used in tagging commits on the main branch and should be of the form v0.1.7 using the semantic versioning convention. Building & deploying the documentation The documentation should build automatically on pushes to main using GitHub actions, if you want to build and deploy the docs manually, follow these steps: Run make build-docs to build the docs to the ./site directory. Then run make deploy-docs to deploy to the gh-pages branch of the repository. You must have write access to the repo.","title":"Development"},{"location":"development/#development","text":"Depends on: Python >=3.9 Clone the repository and create and activate a python virtual environment of your choice. Inside a virtual environment or machine: python -m pip install -r requirements.txt","title":"Development"},{"location":"development/#releaseversioning","text":"Version numbers should be used in tagging commits on the main branch and should be of the form v0.1.7 using the semantic versioning convention.","title":"Release/Versioning"},{"location":"development/#building-deploying-the-documentation","text":"The documentation should build automatically on pushes to main using GitHub actions, if you want to build and deploy the docs manually, follow these steps: Run make build-docs to build the docs to the ./site directory. Then run make deploy-docs to deploy to the gh-pages branch of the repository. You must have write access to the repo.","title":"Building &amp; deploying the documentation"},{"location":"how-polarroute-pipeline-works/","text":"How PolarRoute-pipeline works PolarRoute-pipeline creates one or more meshes using up-to-date source datasets. These meshes can then have optimised routes calculated upon them with optimisations such as fuel or traveltime. The logical flow of the pipeline is built from the application.yaml config file, which details what tasks are to be performed and their dependencies. The pipeline resolves these dependencies and allows tasks to run in parallel, depending how many WORKERS have been defined in the pipeline.yaml . If any task fails to complete successfully, it will raise an exception and prevent any future dependencies from executing. The workflow manager The pipeline build command creates or re-creates from the application.yaml and pipeline.yaml a python script that is used by the Jug parallelisation package. The pipeline invokes 'Jug' with this python script for each WORKER , creating one or more parallel processes that can complete multiple tasks which being monitored. This collection of python script, 'Jug' and WORKERS is referred to as the 'workflow-manager'. Everything related to the workflow manager's operation is contained within the <pipeline>/workflow-manager/ directory, which is created by the 'build' command. pipeline.yaml This configuration file can be mostly left untouched other than the MAXWORKERS definition. The workflow manager will attempt to allocate up to this many workers to the pipeline. Example: | You have 10 tasks the could all execute in parallel. | You are using a platform that has 6 CPU threads. * If you set MAXWORKERS to 2 the workflow manager will invoke 2 workers, meaning that the 2 CPU threads can complete all 10 tasks twice as quickly as if there was only 1 worker (i.e. 1 task done at a time). * If you set MAXWORKERS to 10 the workflow manager will invoke 10 workers but because this is more than available CPU threads there will be a significant amount of CPU context switching to achieve the effect of 10 CPU threads running. This results in slower performance. * If you set MAXWORKERS to 5 the workflow manager will invoke 5 workers, meaning that the 5 CPU threads can complete all 10 tasks five times as quickly as if there was only 1 worker (i.e. 1 task done at a time). This would also avoid CPU context switch and also leave 1 CPU thread free for the underlying platform. application.yaml Environment variables If the pipeline relies upon constants held within environment variables, these can be pre-defined under the env:variables: section of the yaml config file. Also note that the PIPELINE_DIRECTORY and SCRIPTS_DIRECTORY are mandatory for the pipeline to know where it is and where to look for the task scripts. Task sequence The sequence order and dependancies of the tasks (scripts) are defined under the sequence: section of the yaml config file. Each task (script) in the sequence has a name: and depends: field. The name is the name of the script to be found in the scripts directory. The depends can be either a single script name or a list of script names if there are multiple dependancies. If a script has no dependancy then the depends: field should contain an empty string '' . Currently shell scripts .sh and python scripts .py are the only supported task (script) names. Inspect the application.yaml to show how the sequence of tasks can be constructed. Logs Logs of stderr and stdout are stored in <pipeline>/logs/<config_name>_<date>.err and <pipeline>/logs/<config_name>_<date>.out for debugging purposes. Further detail For more detail on the inner workings of the tasks PolarRoute-pipeline performs, please refer to the documentation for: - Jug - PolarRoute - MeshiPhi","title":"How PolarRoute-pipeline works"},{"location":"how-polarroute-pipeline-works/#how-polarroute-pipeline-works","text":"PolarRoute-pipeline creates one or more meshes using up-to-date source datasets. These meshes can then have optimised routes calculated upon them with optimisations such as fuel or traveltime. The logical flow of the pipeline is built from the application.yaml config file, which details what tasks are to be performed and their dependencies. The pipeline resolves these dependencies and allows tasks to run in parallel, depending how many WORKERS have been defined in the pipeline.yaml . If any task fails to complete successfully, it will raise an exception and prevent any future dependencies from executing.","title":"How PolarRoute-pipeline works"},{"location":"how-polarroute-pipeline-works/#the-workflow-manager","text":"The pipeline build command creates or re-creates from the application.yaml and pipeline.yaml a python script that is used by the Jug parallelisation package. The pipeline invokes 'Jug' with this python script for each WORKER , creating one or more parallel processes that can complete multiple tasks which being monitored. This collection of python script, 'Jug' and WORKERS is referred to as the 'workflow-manager'. Everything related to the workflow manager's operation is contained within the <pipeline>/workflow-manager/ directory, which is created by the 'build' command.","title":"The workflow manager"},{"location":"how-polarroute-pipeline-works/#pipelineyaml","text":"This configuration file can be mostly left untouched other than the MAXWORKERS definition. The workflow manager will attempt to allocate up to this many workers to the pipeline. Example: | You have 10 tasks the could all execute in parallel. | You are using a platform that has 6 CPU threads. * If you set MAXWORKERS to 2 the workflow manager will invoke 2 workers, meaning that the 2 CPU threads can complete all 10 tasks twice as quickly as if there was only 1 worker (i.e. 1 task done at a time). * If you set MAXWORKERS to 10 the workflow manager will invoke 10 workers but because this is more than available CPU threads there will be a significant amount of CPU context switching to achieve the effect of 10 CPU threads running. This results in slower performance. * If you set MAXWORKERS to 5 the workflow manager will invoke 5 workers, meaning that the 5 CPU threads can complete all 10 tasks five times as quickly as if there was only 1 worker (i.e. 1 task done at a time). This would also avoid CPU context switch and also leave 1 CPU thread free for the underlying platform.","title":"pipeline.yaml"},{"location":"how-polarroute-pipeline-works/#applicationyaml","text":"","title":"application.yaml"},{"location":"how-polarroute-pipeline-works/#environment-variables","text":"If the pipeline relies upon constants held within environment variables, these can be pre-defined under the env:variables: section of the yaml config file. Also note that the PIPELINE_DIRECTORY and SCRIPTS_DIRECTORY are mandatory for the pipeline to know where it is and where to look for the task scripts.","title":"Environment variables"},{"location":"how-polarroute-pipeline-works/#task-sequence","text":"The sequence order and dependancies of the tasks (scripts) are defined under the sequence: section of the yaml config file. Each task (script) in the sequence has a name: and depends: field. The name is the name of the script to be found in the scripts directory. The depends can be either a single script name or a list of script names if there are multiple dependancies. If a script has no dependancy then the depends: field should contain an empty string '' . Currently shell scripts .sh and python scripts .py are the only supported task (script) names. Inspect the application.yaml to show how the sequence of tasks can be constructed.","title":"Task sequence"},{"location":"how-polarroute-pipeline-works/#logs","text":"Logs of stderr and stdout are stored in <pipeline>/logs/<config_name>_<date>.err and <pipeline>/logs/<config_name>_<date>.out for debugging purposes.","title":"Logs"},{"location":"how-polarroute-pipeline-works/#further-detail","text":"For more detail on the inner workings of the tasks PolarRoute-pipeline performs, please refer to the documentation for: - Jug - PolarRoute - MeshiPhi","title":"Further detail"},{"location":"implementation/","text":"Implementation PolarRoute-pipeline is one of three broadly distinct components of the Operational PolarRoute (OPR) project. PolarRoute-pipeline ( this repo ) Required data products are downloaded, stored and processed shoreside where the data and compute resources are abundant. This generates ocean/sea-ice meshes (and routes if required) for areas of operational interest. These generated outputs are compressed and transferred via satellite (or other) link to shipside. PolarRoute-pipeline is built on the technologies of PolarRoute and MeshiPhi . PolarRoute-server Once the meshes are available shipside they are ingested into an onboard database which allows them to be used to calculate optimised travel routes. These are calculated on demand and presented to the onboard user digitally. PolarRoute-server handles the onboard mesh ingestion and route calculation through a command-line or graphical user interface. Sea-Ice-Information-System (SIIS) The SIIS front-end provides a graphical user interface whereby users can interact with Operational PolarRoute through a standard web browser. From the three distinct components defined above, this documentation is concerned only with part (1.) Basic process flow diagram Detailed process flow diagram History During 2024 the previous repository was manually ported from a hierarchy of bash and python scripts to an implementation using a workflow manager. The original scripts remained mostly unchanged although the introduction of the Jug parallelisation package allowed the scripts to execute with strict dependency, monitoring and pipeline control. This was initially achieved using a handbuilt workflow manager script operational-polarroute.py . September 2024 onwards The workflow manager concept was rebuilt as a separate and generic workflow manager forming the package simple-action-pipeline . PolarRoute-pipeline was then ported to being a configuration of simple-action-pipeline .","title":"Implementation"},{"location":"implementation/#implementation","text":"PolarRoute-pipeline is one of three broadly distinct components of the Operational PolarRoute (OPR) project. PolarRoute-pipeline ( this repo ) Required data products are downloaded, stored and processed shoreside where the data and compute resources are abundant. This generates ocean/sea-ice meshes (and routes if required) for areas of operational interest. These generated outputs are compressed and transferred via satellite (or other) link to shipside. PolarRoute-pipeline is built on the technologies of PolarRoute and MeshiPhi . PolarRoute-server Once the meshes are available shipside they are ingested into an onboard database which allows them to be used to calculate optimised travel routes. These are calculated on demand and presented to the onboard user digitally. PolarRoute-server handles the onboard mesh ingestion and route calculation through a command-line or graphical user interface. Sea-Ice-Information-System (SIIS) The SIIS front-end provides a graphical user interface whereby users can interact with Operational PolarRoute through a standard web browser. From the three distinct components defined above, this documentation is concerned only with part (1.)","title":"Implementation"},{"location":"implementation/#basic-process-flow-diagram","text":"","title":"Basic process flow diagram"},{"location":"implementation/#detailed-process-flow-diagram","text":"","title":"Detailed process flow diagram"},{"location":"implementation/#history","text":"During 2024 the previous repository was manually ported from a hierarchy of bash and python scripts to an implementation using a workflow manager. The original scripts remained mostly unchanged although the introduction of the Jug parallelisation package allowed the scripts to execute with strict dependency, monitoring and pipeline control. This was initially achieved using a handbuilt workflow manager script operational-polarroute.py .","title":"History"},{"location":"implementation/#september-2024-onwards","text":"The workflow manager concept was rebuilt as a separate and generic workflow manager forming the package simple-action-pipeline . PolarRoute-pipeline was then ported to being a configuration of simple-action-pipeline .","title":"September 2024 onwards"},{"location":"installation/","text":"Installing the pipeline It is recommended to use a Python virtual environment to reduce the risk of any Python package conflicts. HPC Workstation or Local PC Create a Python virtual environment The Python version must be Python 3.9 or higher (3.12 was used during development). Check the available Python with python --version If required, install or load a compatible python version. Your HPC administrator will be able to help with getting a compatible Python version. then python -m venv <path-to-venv> with a path of your choosing. Source the new newly created python venv source <path-to-venv>/bin/activate (Assuming you're using Bash or similar. Use the appropriate activate script within that folder depending on your shell) Clone this repository Assuming you have already cloned this repository into a directory, move into the 'root' of this repository. cd polarroute-pipeline . Otherwise git clone https://github.com/bas-amop/PolarRoute-pipeline.git polarroute-pipeline then cd polarroute-pipeline Install requirements Using python pip (inside the created venv) python -m pip install -r requirements.txt","title":"Installing the pipeline"},{"location":"installation/#installing-the-pipeline","text":"It is recommended to use a Python virtual environment to reduce the risk of any Python package conflicts.","title":"Installing the pipeline"},{"location":"installation/#hpc-workstation-or-local-pc","text":"Create a Python virtual environment The Python version must be Python 3.9 or higher (3.12 was used during development). Check the available Python with python --version If required, install or load a compatible python version. Your HPC administrator will be able to help with getting a compatible Python version. then python -m venv <path-to-venv> with a path of your choosing. Source the new newly created python venv source <path-to-venv>/bin/activate (Assuming you're using Bash or similar. Use the appropriate activate script within that folder depending on your shell) Clone this repository Assuming you have already cloned this repository into a directory, move into the 'root' of this repository. cd polarroute-pipeline . Otherwise git clone https://github.com/bas-amop/PolarRoute-pipeline.git polarroute-pipeline then cd polarroute-pipeline Install requirements Using python pip (inside the created venv) python -m pip install -r requirements.txt","title":"HPC Workstation or Local PC"},{"location":"setting-up/","text":"Setting up the pipeline Even before the pipeline can be built for the first time, there are a number of one-time setup steps required. Assuming you have already created a Python virtual environment and cloned this repository into a directory on a HPC Workstation or Local PC, move into the 'root' of the repository. cd polarroute-pipeline Create symbolic links for the venv activation script, datastore (where downloaded data products are to be stored), logs , outputs (where the generated outputs are to be stored), html (for the summary status page) and upload + push (where outputs are copied to be sent shipside). ln -s <path-to-venv>/bin/activate <path-to-this-repo>/activate ln -s <path-to-datastore> <path-to-this-repo>/datastore ln -s <path-to-logs-directory> <path-to-this-repo>/logs ln -s <path-to-output-archive> <path-to-this-repo>/outputs ln -s <path-to-upload-directory> <path-to-this-repo>/upload ln -s <path-to-push-directory> <path-to-this-repo>/push ln -s <path-to-html-directory> <path-to-this-repo>/html The links created above are specific to PolarRoute-pipeline as various data products are stored in differen't remote or local directories. If you are setting up a completely local instance of PolarRoute-pipeline then you could just create local folders within the pipeline directory, instead of links to external locations. Below is an explanation of why each link/directory is required: Directory or Link Purpose <pipeline>/activate So the pipeline knows which activation script to use <pipeline>/datastore Where to store and retrieve downloaded source datasets <pipeline>/logs Where to keep any log files <pipeline>/outputs Where to store and retrieve daily pipeline output products <pipeline>/upload Where to 'prepare' specific outputs before being sent <pipeline>/push Where to place any outputs to be sent. Specifically, the pipeline copies output products from the upload directory into the push directory. These are then picked up by an external synchronisation system which 'pulls' the products and automatically removes them from the push directory afterwards <pipeline>/html Where the pipeline publishes a static html summary page Setting up download credentials PolarRoute-pipeline will need to use valid credentials to download ERA5 and DUACS products, ensure you have these set up as detailed below: ERA5 The ERA5 downloader scripts make use of the CDS API (via the cdsapi python package) and require you to create a .cdsapirc file in your home directory ($HOME/.cdsapirc) containing a valid url and key for the API as described here: https://cds.climate.copernicus.eu/api-how-to From a shell: echo url: https://cds-beta.climate.copernicus.eu/api > $HOME/.cdsapirc echo key: <your-unique-api-key> >> $HOME/.cdsapirc echo verify:0 >> $HOME/.cdsapirc Copernicus Marine API The Copernicus API to is used to download up-to-date DUACS currents data. This service requires obtaining a USERNAME and PASSWORD for logging in. Once you have the username and password they can be stored separately to the pipeline in the user's HOME directory. You can register on the Copernicus Marine API Registration page. Then, use the copernicusmarine command line tool to log in and set up your credentials file. First make sure that your python virtual environment is activated and you have installed the dependencies. Then: copernicusmarine login # you will be prompted for your username and password and your credentials will be stored in a file at $HOME/.copernicusmarine/.copernicusmarine-credentials Alternativey, Copernicus Marine credentials can be set using environment variables COPERNICUSMARINE_SERVICE_USERNAME and COPERNICUSMARINE_SERVICE_PASSWORD - these will be used in preference to the credentials file. Now that everything is set up, the PolarRoute-pipeline can be used. Please refer to the Using the pipeline section of this documentation for details of how to operate the pipeline. GEBCO Bathymetry data If you are running the pipeline locally, and do not have access to the BAS infrastructure (specifically the SAN), you can use the following script to download and set up the GEBCO gridded bathymetry data : mkdir -p datastore/bathymetry/gebco && cd $_ # Make a request using wget - this can take a while to download # as this bathymetry model can be greater than 7GB in size. # Take note of the year here, newer versions may be available wget -O gebco.zip https://www.bodc.ac.uk/data/open_download/gebco/gebco_2024/zip/ unzip gebco.zip mv GEBCO_2024.nc gebco_global.nc # Clean up, remove any unnecessary files rm gebco.zip rm *.pdf This is a large and static dataset, therefore you should only need to run this once.","title":"Setting up the pipeline"},{"location":"setting-up/#setting-up-the-pipeline","text":"Even before the pipeline can be built for the first time, there are a number of one-time setup steps required. Assuming you have already created a Python virtual environment and cloned this repository into a directory on a HPC Workstation or Local PC, move into the 'root' of the repository. cd polarroute-pipeline Create symbolic links for the venv activation script, datastore (where downloaded data products are to be stored), logs , outputs (where the generated outputs are to be stored), html (for the summary status page) and upload + push (where outputs are copied to be sent shipside). ln -s <path-to-venv>/bin/activate <path-to-this-repo>/activate ln -s <path-to-datastore> <path-to-this-repo>/datastore ln -s <path-to-logs-directory> <path-to-this-repo>/logs ln -s <path-to-output-archive> <path-to-this-repo>/outputs ln -s <path-to-upload-directory> <path-to-this-repo>/upload ln -s <path-to-push-directory> <path-to-this-repo>/push ln -s <path-to-html-directory> <path-to-this-repo>/html The links created above are specific to PolarRoute-pipeline as various data products are stored in differen't remote or local directories. If you are setting up a completely local instance of PolarRoute-pipeline then you could just create local folders within the pipeline directory, instead of links to external locations. Below is an explanation of why each link/directory is required: Directory or Link Purpose <pipeline>/activate So the pipeline knows which activation script to use <pipeline>/datastore Where to store and retrieve downloaded source datasets <pipeline>/logs Where to keep any log files <pipeline>/outputs Where to store and retrieve daily pipeline output products <pipeline>/upload Where to 'prepare' specific outputs before being sent <pipeline>/push Where to place any outputs to be sent. Specifically, the pipeline copies output products from the upload directory into the push directory. These are then picked up by an external synchronisation system which 'pulls' the products and automatically removes them from the push directory afterwards <pipeline>/html Where the pipeline publishes a static html summary page","title":"Setting up the pipeline"},{"location":"setting-up/#setting-up-download-credentials","text":"PolarRoute-pipeline will need to use valid credentials to download ERA5 and DUACS products, ensure you have these set up as detailed below:","title":"Setting up download credentials"},{"location":"setting-up/#era5","text":"The ERA5 downloader scripts make use of the CDS API (via the cdsapi python package) and require you to create a .cdsapirc file in your home directory ($HOME/.cdsapirc) containing a valid url and key for the API as described here: https://cds.climate.copernicus.eu/api-how-to From a shell: echo url: https://cds-beta.climate.copernicus.eu/api > $HOME/.cdsapirc echo key: <your-unique-api-key> >> $HOME/.cdsapirc echo verify:0 >> $HOME/.cdsapirc","title":"ERA5"},{"location":"setting-up/#copernicus-marine-api","text":"The Copernicus API to is used to download up-to-date DUACS currents data. This service requires obtaining a USERNAME and PASSWORD for logging in. Once you have the username and password they can be stored separately to the pipeline in the user's HOME directory. You can register on the Copernicus Marine API Registration page. Then, use the copernicusmarine command line tool to log in and set up your credentials file. First make sure that your python virtual environment is activated and you have installed the dependencies. Then: copernicusmarine login # you will be prompted for your username and password and your credentials will be stored in a file at $HOME/.copernicusmarine/.copernicusmarine-credentials Alternativey, Copernicus Marine credentials can be set using environment variables COPERNICUSMARINE_SERVICE_USERNAME and COPERNICUSMARINE_SERVICE_PASSWORD - these will be used in preference to the credentials file. Now that everything is set up, the PolarRoute-pipeline can be used. Please refer to the Using the pipeline section of this documentation for details of how to operate the pipeline.","title":"Copernicus Marine API"},{"location":"setting-up/#gebco-bathymetry-data","text":"If you are running the pipeline locally, and do not have access to the BAS infrastructure (specifically the SAN), you can use the following script to download and set up the GEBCO gridded bathymetry data : mkdir -p datastore/bathymetry/gebco && cd $_ # Make a request using wget - this can take a while to download # as this bathymetry model can be greater than 7GB in size. # Take note of the year here, newer versions may be available wget -O gebco.zip https://www.bodc.ac.uk/data/open_download/gebco/gebco_2024/zip/ unzip gebco.zip mv GEBCO_2024.nc gebco_global.nc # Clean up, remove any unnecessary files rm gebco.zip rm *.pdf This is a large and static dataset, therefore you should only need to run this once.","title":"GEBCO Bathymetry data"},{"location":"using/","text":"Using the pipeline If using a freshly installed pipeline for the first time, then the pipeline must be built from the provided pipeline and application yaml files. This also applies if changes have been made to either yaml file, i.e. the pipeline build command must be issued for any yaml configuration changes to take effect. make sure your virtual environment is activated before issuing any of the commands below source <path-to-this-pipeline-directory>\\activate Build the pipeline build To build the pipeline from the provided pipeline.yaml and application.yaml , run the command: pipeline build <path-to-this-pipeline-directory> Get the pipeline status status As well as checking the status, it can also be used to check that the pipeline is installed and setup correctly. This can be done by running the pipeline's status command: pipeline status <path-to-this-pipeline-directory> # or for the short output pipeline status <path-to-this-pipeline-directory> --short A long (or short) report should be output. This status command can be run at any time and will give the 'live' state of the pipeline and it's tasks. The status of the pipeline is stateful and persistent, even after the execution is completed, which is useful for querying long after the pipeline has completed. This holds true if the pipeline fails for any reason. Execute the pipeline execute To start the pipeline, run the command: pipeline execute <path-to-this-pipeline-directory> Reset the pipeline reset Because the statefulness of the pipeline persists even after completion, an additional step is required before the pipeline can be executed again. This is called a reset , and when initiated, the workflow manager erases the state of the pipeline ready for re-execution. A reset can be performed by running the command: pipeline reset <path-to-this-pipeline-directory> Resetting a running pipeline is not advised and may produce unpredictable behaviour (please refer to halt below). Halt all pipeline tasks halt If the pipeline needs to be halted whilst it is running, the halt command has been provided. pipeline halt <path-to-this-pipeline-directory> This does not erase the statefulness of the pipeline, so the status command can be used after halting has occured. Any pipeline tasks that have already completed will remain so, although any tasks which haven't fully completed will revert to being not started. Following a 'halt' there are two possible choices: execute will resume the pipeline from where it was halted. reset will reset the pipeline to it's un-executed state. Tips Using any of the pipeline commands does not require sourcing of the pipeline's pipeline.env and application.env files beforehand, this is automatically handled by the pipeline. Running the pipeline command from inside a pipeline directory does not require specifying the <path-to-pipeline-directory> argument. When this argument is missing, the pipeline assumes the use of the current working directory. For instance, if you are inside the pipeline directory, you can simply issue the command pipeline status to get the current status.","title":"Using the pipeline"},{"location":"using/#using-the-pipeline","text":"If using a freshly installed pipeline for the first time, then the pipeline must be built from the provided pipeline and application yaml files. This also applies if changes have been made to either yaml file, i.e. the pipeline build command must be issued for any yaml configuration changes to take effect. make sure your virtual environment is activated before issuing any of the commands below source <path-to-this-pipeline-directory>\\activate","title":"Using the pipeline"},{"location":"using/#build-the-pipeline-build","text":"To build the pipeline from the provided pipeline.yaml and application.yaml , run the command: pipeline build <path-to-this-pipeline-directory>","title":"Build the pipeline build"},{"location":"using/#get-the-pipeline-status-status","text":"As well as checking the status, it can also be used to check that the pipeline is installed and setup correctly. This can be done by running the pipeline's status command: pipeline status <path-to-this-pipeline-directory> # or for the short output pipeline status <path-to-this-pipeline-directory> --short A long (or short) report should be output. This status command can be run at any time and will give the 'live' state of the pipeline and it's tasks. The status of the pipeline is stateful and persistent, even after the execution is completed, which is useful for querying long after the pipeline has completed. This holds true if the pipeline fails for any reason.","title":"Get the pipeline status status"},{"location":"using/#execute-the-pipeline-execute","text":"To start the pipeline, run the command: pipeline execute <path-to-this-pipeline-directory>","title":"Execute the pipeline execute"},{"location":"using/#reset-the-pipeline-reset","text":"Because the statefulness of the pipeline persists even after completion, an additional step is required before the pipeline can be executed again. This is called a reset , and when initiated, the workflow manager erases the state of the pipeline ready for re-execution. A reset can be performed by running the command: pipeline reset <path-to-this-pipeline-directory> Resetting a running pipeline is not advised and may produce unpredictable behaviour (please refer to halt below).","title":"Reset the pipeline reset"},{"location":"using/#halt-all-pipeline-tasks-halt","text":"If the pipeline needs to be halted whilst it is running, the halt command has been provided. pipeline halt <path-to-this-pipeline-directory> This does not erase the statefulness of the pipeline, so the status command can be used after halting has occured. Any pipeline tasks that have already completed will remain so, although any tasks which haven't fully completed will revert to being not started. Following a 'halt' there are two possible choices: execute will resume the pipeline from where it was halted. reset will reset the pipeline to it's un-executed state.","title":"Halt all pipeline tasks halt"},{"location":"using/#tips","text":"Using any of the pipeline commands does not require sourcing of the pipeline's pipeline.env and application.env files beforehand, this is automatically handled by the pipeline. Running the pipeline command from inside a pipeline directory does not require specifying the <path-to-pipeline-directory> argument. When this argument is missing, the pipeline assumes the use of the current working directory. For instance, if you are inside the pipeline directory, you can simply issue the command pipeline status to get the current status.","title":"Tips"}]}